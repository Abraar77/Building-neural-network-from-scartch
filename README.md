ğŸ§  Two-Layer Neural Network from Scratch (NumPy)

This repository contains a from-scratch implementation of a 2-layer neural network using only NumPy, without relying on high-level frameworks like TensorFlow or PyTorch.

The goal of this project is to understand the mathematics and mechanics of neural networks, including forward propagation, backpropagation, and gradient descent.

ğŸ“Œ Model Architecture
Input Layer (784 features)
        â†“
Hidden Layer (10 neurons, ReLU)
        â†“
Output Layer (10 neurons, Softmax)


Input: 28Ã—28 grayscale images (flattened â†’ 784 features)

Hidden layer: ReLU activation

Output layer: Softmax for multi-class classification (digits 0â€“9)

ğŸ§® Mathematical Overview
Forward Propagation
ğ‘
1
=
ğ‘Š
1
ğ‘‹
+
ğ‘
1
Z
1
	â€‹

=W
1
	â€‹

X+b
1
	â€‹

ğ´
1
=
ReLU
(
ğ‘
1
)
A
1
	â€‹

=ReLU(Z
1
	â€‹

)
ğ‘
2
=
ğ‘Š
2
ğ´
1
+
ğ‘
2
Z
2
	â€‹

=W
2
	â€‹

A
1
	â€‹

+b
2
	â€‹

ğ´
2
=
Softmax
(
ğ‘
2
)
A
2
	â€‹

=Softmax(Z
2
	â€‹

)
Loss Function

Categorical Cross-Entropy (Log Loss)

ğ¿
=
âˆ’
1
ğ‘š
âˆ‘
ğ‘¦
log
â¡
(
ğ‘¦
^
)
L=âˆ’
m
1
	â€‹

âˆ‘ylog(
y
^
	â€‹

)
Backpropagation

Gradients are computed using the chain rule:

ğ‘‘
ğ‘
2
=
ğ´
2
âˆ’
ğ‘Œ
one-hot
dZ
2
	â€‹

=A
2
	â€‹

âˆ’Y
one-hot
	â€‹

ğ‘‘
ğ‘Š
2
=
1
ğ‘š
ğ‘‘
ğ‘
2
ğ´
1
ğ‘‡
dW
2
	â€‹

=
m
1
	â€‹

dZ
2
	â€‹

A
1
T
	â€‹

ğ‘‘
ğ‘
2
=
1
ğ‘š
âˆ‘
ğ‘‘
ğ‘
2
db
2
	â€‹

=
m
1
	â€‹

âˆ‘dZ
2
	â€‹

ğ‘‘
ğ‘
1
=
ğ‘Š
2
ğ‘‡
ğ‘‘
ğ‘
2
âŠ™
ReLU
â€²
(
ğ‘
1
)
dZ
1
	â€‹

=W
2
T
	â€‹

dZ
2
	â€‹

âŠ™ReLU
â€²
(Z
1
	â€‹

)
ğ‘‘
ğ‘Š
1
=
1
ğ‘š
ğ‘‘
ğ‘
1
ğ‘‹
ğ‘‡
dW
1
	â€‹

=
m
1
	â€‹

dZ
1
	â€‹

X
T
ğ‘‘
ğ‘
1
=
1
ğ‘š
âˆ‘
ğ‘‘
ğ‘
1
db
1
	â€‹

=
m
1
	â€‹

âˆ‘dZ
1
	â€‹

ğŸ› ï¸ Implementation Details

Language: Python

Library: NumPy only

No ML frameworks used

Fully vectorized operations

Manual implementation of:

ReLU

Softmax

One-hot encoding

Backpropagation

Gradient descent

ğŸ“‚ Code Structure
.
â”œâ”€â”€ init_params()        # Initialize weights & biases
â”œâ”€â”€ forward_prop()       # Forward propagation
â”œâ”€â”€ backward_prop()      # Backpropagation
â”œâ”€â”€ update_params()      # Gradient descent update
â”œâ”€â”€ one_hot()            # Label encoding
â””â”€â”€ train loop

ğŸš€ Training Loop (High Level)
for epoch in range(epochs):
    Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)
    dW1, db1, dW2, db2 = backward_prop(...)
    W1, b1, W2, b2 = update_params(...)

ğŸ¯ Key Learning Outcomes

Why activation functions are required

Why matrix dimensions matter

How gradients flow backward

Why loss is averaged using 1/m

How a neural network actually learns

âš ï¸ Notes

This is an educational implementation, not optimized for speed

Initialization uses simple random values (no Xavier/He init)

Bias gradients are averaged across samples

ğŸ“Œ Why This Project?

Most beginners use deep learning libraries without understanding what happens underneath.
This project focuses on building intuition and mathematical clarity by implementing everything manually.

Note: A TensorFlow implementation of the same model is also included purely for comparison, to highlight how deep learning frameworks automate the same forward and backward propagation implemented manually in NumPy. 

 N-layer neural network has also been uploaded check my repos

ğŸ‘¤ Author

Abraar
GitHub: Abraar77
